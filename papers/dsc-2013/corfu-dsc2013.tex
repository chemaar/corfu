\documentclass{llncs}

%\usepackage{llncsdoc}

%\usepackage{makeidx}  % allows for indexgeneration
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{multirow}

\usepackage{url}
\usepackage{rotating}

%%%Math
\usepackage{latexsym}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{amsthm}
%\usepackage{eurosans}

\usepackage{eurosym}

\usepackage{longtable}

\usepackage{listings}

\usepackage{color}
\usepackage{textcomp}

\definecolor{darkred}{rgb}{0.5, 0, 0}
\definecolor{violet}{rgb}{1, 0, 1}
\definecolor{green}{rgb}{0.3, 0.95, 0.3}
\definecolor{listinggray}{gray}{0.97}



\begin{document}
\title{A natural language and semantic-based technique to unify corporate names in the e-Procurement sector. The CORFU approach.\thanks{THANKS}}

\titlerunning{}

\author{Jose Mar\'{i}a Alvarez-Rodr\'{i}guez\inst{1}} 

\authorrunning{Jose Mar\'{i}a Alvarez-Rodr\'{i}guez}


\tocauthor{Jose Mar\'{i}a Alvarez}


\institute{The South East European Research Center\\   
  \email{{jmalvarez@seerc.org}},\\
   WWW home page: \texttt{http://www.seerc.org}, \\
}


\date{}

\maketitle

\renewcommand{\labelitemi}{$\bullet$}

\begin{abstract}
Public administrations are currently facing a big challenge trying to improve both the peformance and the transparency of administrative processes.
In this context the e-Government and Open Linked Data initiatives are tackling existing interoperability and 
integration issues among ICT-based systems but the creation of a real transparent environment requires 
much more than the simple publication of data and information in specific open formats; data and information 
quality is the next major step in the pubic sector. More specifically in the e-Procurement domain there is a 
vast amount of valuable data that is already available via the Internet protocols and formats and can be used 
for the creation of new added-value services. Neverthless the simple extraction of statistics or creation of reports 
can imply extra tasks with regards to clean, prepare and reconcile data. 
From a transparency point of view one of the most interesting services lies in tracking rewarded contracts (type, location, and supplier). 
Depending on the capabilities of the public organization this kind of basic report can turn into a 
complex task due to a lack of standardization in supplier names or the use of different descriptors for the type of contract. That is why 
this paper presents a stepwise method based on natural language processing and semantics to deal with the hetereogenities in corporate names. 
Furthermore a research study to evaluate the precision and recall of the proposed technique, using as use case the public dataset of rewarded public 
contracts in Australia during the period 2005-2012, is also presented, finally some discussion, conclusions and future work 
are also outlined.
\end{abstract}

\section{Introduction}
Public bodies are continuously publishing procurement opportunities in which 
valuable metadata is available. Depending on the stage of the process new data arises such 
as the supplier name that has been rewarded with the public contract. In this 
context the extraction of statistics on how many contracts have been 
rewarded to the same company is a relevant indicator to evaluate the transparency 
of the whole process. Although companies that want to tender for a public contract must be 
oficially registered and have an unique identification number, the truth is 
that in most of rewarded contracts the supplier is only identified by a name or a string literal typed 
by a civil-servant. In this sense there is not usually a connection between 
the oficial company registry and the process of rewarding contracts implying different 
naming problems and inconsistenty in data that are spread to next stages preventing future 
activities such as reporting.

In the case of the type of contract and location, there are already standardized~\cite{DBLP:journals/ijseke/AlvarezLSASL12} product 
scheme classifications such as the Common Procurement Vocabulary (2003 and 2008), the Combined Nomenclature (2012), 
the Central Product Classificationby by the European Union, the International Standard Industrial Classification of 
All Economic Activities (Rev. 4) by the United Nations or the North American Industry Classification System (2007 and 2012) 
by the Government of United States among others that are used with different objectives such as statistics, tagging or 
information retrieval. Geolocated information can be also found in different common datasets and nomenclatures such as 
the Nomenclature of territorial units for statistics in the European Union, the Geonames dataset, the GeoLinkedData 
initiative or the traditional list of countries and ISO-codes.

However organization, corporate, company or institution names (hereafter these names will be used to refer to 
the same entity) and structure is not yet standardized at global scope and only some classification of economic activities or 
company identifiers can be found. Thus the simple task of grouping contracts by a supplier is not a mere process of 
searching by the same literal. Technical issues such as hyphenation, use of abbreviations or acronynms an transliteration are common p
roblems that must be addressed in order to provide a final corporate name and have been widely studied in the field of 
Name Entity Recognition~\cite{citeulike:1657521} (NER) or name entity disambiguation~\cite{Sarmento:2009:AWN:1602022.1602085,Klein:2003:NER:1119176.1119204}. 
Neverthless the problem that is being tackled in these approaches lies in the identification of organization names in 
a raw text while in the e-Procurement sector the string literal identifying a supplier is already known.

In the particular case of the Australian e-Procurement domain, the supplier name is introduced by typing a string literal without any assistance or 
auto-complete method. Obviously a variety of errors and variants for the same company, see Table~\ref{tabla:aus-suppliers}, 
can be found such as misspelling errors~\cite{NorvigSpelling,StanfordSpelling}, name and acronym mismatches~\cite{Yeates99automaticextraction,Ratinov:2004:AES:1025132.1026366} 
or context-aware data that is already known when the dataset is processed, e.g. country or year. Furthermore it is also well-known 
that a large company can be divided into several divisions or departments but from a statistical point of view grouping data by a supplier name 
should take into account all rewarded contracts regardless the structure of the company.


% In the specific case of company names there is an open database, OpenCorporates, that has collected more than $52$ million of names 
% around the world and it can be considered a perfect candidate to perform the reconciliation process between a string literal and a 
% target resource to obtain an unique identifier. N

On the other hand the application of semantic technologies and the Linking Open Data initiative (hereafter LOD)~\cite{Berners-Lee-2006,Heath_Bizer_2011}  
in several fields like e-Government (e.g. the Open Government Data effort) tries to improve the knowledge about a specific area providing 
common data models and formats to share information and data between agents. More specifically, in the European e-Procurement 
context~\cite{e-Proc-map-paper} there is an increasing commitment to boost the use of electronic communications and transactions 
processing by government institutions and other public sector organizations in order to provide added-value services with special focus on SMEs. 
More specifically the LOD initiative seeks for creating a public data realm in which one the principles of this initiative that lies in the 
unique identification or resources through URIs can become real. Thus entity reconciliation techniques~\cite{Serimi,conf/www/MaaliCP11} 
coming from the ontology mapping and alignment areas or algorithms based on Natural Language Processing (hereafter NLP) have been 
designed to link similar resources already available in different vocabularies, datasets or databases such as DBPedia or Freebase. 
Nevertheless the issue of unifying supplier names as a human would do faces new problems that have been tackled in 
other research works~\cite{Galvez2006} to extract statistics of performance in bibliographic databases. The main objective is not just a 
mere reconciliation process to link to existing resources but to create a unique name or link ($n$ string literals $\to$ $1$ company $\to$ $1$ URI). 
For instance in the case of the ongoing example the string literals ``Oracle'' and ``Oracle University'' could be respectively aligned to the entity $<$Oracle\_Corporation$>$ and $<$Oracle\_University$>$ but 
the problem of grouping by a unique (\textit{Big}) name, identifier or resource still remains. That is why a context-aware method based on NLP 
techniques combined with semantics has been designed, customized and implemented trying to exploit the naming convention of a specific dataset with the aim 
of grouping $n$ string literals $\to$ $1$ company and, thus, easing the next natural process of entity reconciliation.

The remainder of this paper is structured as follows. Section 2 a literature review is presented. Afterwards next section outlines main mismatches in corporate names. Section 4 presents 
the CORFU approach to unify corporate names. The evaluation section exposes and discusses the experimentation carried out to test the presented approach using as a dataset the rewarded 
contracts of Australia in the period 2005-2012. Finally conclusions summarizes the main outcomes of this work 
and some open issues are also presented as future work.



\begin{table}[!htb]
\renewcommand{\arraystretch}{1.3}
\begin{center}
\begin{tabular}{|p{7cm}|p{7cm}|}
\hline
  \textbf{Raw Supplier Name} & \textbf{Target (potential) Supplier Name and URI}  \\  \hline
  ``Accenture'' & \multirow{12}{*}{``Accenture''} \\
  ``ACCENTURE'' & \multirow{12}{*}{\url{http://live.dbpedia.org/resource/Accenture}} \\ 
  ``Accenture Aust Holdings'' & \\  
  ``Accenture Aust Holdings Pty Ltd'' & \\
  ``Accenture Australia'' & \\
  ``ACCENTURE AUSTRALIA'' & \\
  ``Accenture  Australia Holding P/L'' & \\
  ``Accenture Australia Holdings P/Ltd'' & \\
  ``Accenture Australia Holdings Pty'' & \\
  ``Accenture Australia Holdings Pty Lt''  & \\  
  ``Accenture Australia Limited'' & \\
  \ldots  & \\
  ``Accenture Australia Ltd'' & \\ \hline
  ``Microsoft Australia'' & \multirow{3}{*}{``Microsoft''} \\
  ``Microsoft Australia Pty Ltd'' & \multirow{3}{*}{\url{http://live.dbpedia.org/resource/Microsoft}} \\
  \ldots  & \\
  ``Microsoft Enterprise Services'' & \\ \hline
  ``Oracle (Corp) Aust Pty Ltd''  & \multirow{14}{*}{``Oracle''} \\
  ``Oracle Corp (Aust) Pty Ltd''  & \multirow{14}{*}{\url{http://live.dbpedia.org/resource/Oracle_Corporation}} \\
  ``Oracle Corp Aust Pty Ltd'' & \\
  ``ORACLE CORP AUST PTY LTD'' & \\
  ``Oracle Corp. Australia'' & \\
  ``ORACLE CORP AUSTRALIA P/L'' & \\
  ``Oracle Corp. Australia Pty.Ltd.'' & \\
  ``ORACLE CORP AUSTRALIA PTY LTD'' & \\
  ``Oracle Corpoartion (Aust) Pty Ltd'' & \\
  ``Oracle Corporate Aust Pty Ltd'' & \\
  ``Oracle Corporation'' & \\
  ``Oracle Risk Consultants'' & \\
  ``ORACLE SYSTEMS (AUSTRALIA) PTY LTD'' & \\
  \ldots  & \\
  ``Oracle University''  & \\ \hline
  ``PRICEWATERHOUSECOOPERS(PWC)''  & \multirow{8}{*}{``PricewaterhouseCoopers''} \\
  ``PricewaterhouseCoopers Securities Ltd''& \multirow{8}{*}{\url{http://dbpedia.org/resource/PricewaterhouseCoopers}} \\
  ``PriceWaterhouseCoopers Securities Ltd'' & \\
  ``PRICEWATERHOUSE COOPERS SECURITIES LTD'' & \\
  ``PricewaterhouseCoopers Services LLP'' & \\
  ``Pricewaterhousecoopers Services Pty Ltd'' & \\
  ``PriceWaterhouseCoopers (T/A: PriceWaterhouseCoopers Legal)'' & \\
  \ldots  & \\
  ``Pricewaterhouse (PWC)'' & \\ \hline
  \ldots & \ldots \\
  \hline
  \end{tabular}
  \caption{Examples of supplier names in Australian rewarded contracts.}
  \label{tabla:aus-suppliers}
  \end{center}
\end{table} 


\section{Related Work}
The literature review of this review covers the following areas:
\begin{itemize}
 \item Natural Language Processing and Computational Linguistics. In these research areas common works dealing with the aforementioned data hetereogenities 
   such as misspelling errors~\cite{NorvigSpelling,StanfordSpelling} and name/acronym mismatches~\cite{Yeates99automaticextraction,Ratinov:2004:AES:1025132.1026366}, 
  in the lexical, syntactic and semantic levels can be found. These approaches can be applied to solve general problems and usually follow a 
  traditional approach of text normalization, lexical analysis, pos-tagging word according to a grammar and semantic analysis to filter or 
  provide some kind of service such as information/knowledge extraction, reporting, sentiment analysis or opinion mining. 
  Well-stablished APIs such as NLTK~\cite{LoperBird02} for Python, Lingpipe~\cite{Lingpipe}, OpenNLP~\cite{OpenNLP} or Gate~\cite{Gate} for Java, WEKA~\cite{read12:_scalab} 
  (a data mining library with NLP capabilities), the Apache Lucene and Solr~\cite{rafa2011apache} search engines provide the proper building blocks to build natural-language based applications. 
  Recent times have seen how the analysis of social networks such as Twitter~\cite{Li:2012:TNE:2348283.2348380,Gimpel:2011:PTT:2002736.2002747}, the extraction of 
  clinical terms~\cite{Wang:2009:ARN:1667884.1667888} for electronic health records, the creation of bibliometrics~\cite{Galvez2006,Morillo:2013:TAA:2424697.2424727} or 
  the idenfication of gene names~\cite{Krauthammer:2004:TIB:1053007.1053018,Galvez2012} to name a few have tackled the problem of entity recognition and extraction from raw sources. 
  Other supervised techniques~\cite{Bohn:2006:PHD} have also be used to train data mining-based algorithms with the aim of creation classifiers.
 
 \item Semantic Web. More specifically in the LOD initiative~\cite{Berners-Lee-2006} the use of entity reconciliation techniques to uniquely identify resources 
 is being currently explored. Thus an entity reconciliation pocess can be briefly defined as the method for looking and mapping~\cite{DBLP:conf/semweb/IseleJB10,DBLP:conf/icwe/IseleJB12} two different 
 concepts or entities under a certain threshold. There are a lot of  works presenting solutions about concept mapping, entity reconciliation, etc. 
 most of them are focused on the previous NLP techniques~\cite{conf/www/MaaliCP11,Serimi} (if two concepts have similar literal descriptions then they should be similar) 
 and others (ontology-based) that also exploit the semantic information (hierarchy, number and type of relations) to establish a potential mapping 
 (if two concepts share similar properties and similar super classes then these concepts should be similar). Apart from that 
 there are also machine learning techniques to deal with these mismatches in descriptions using statistical approaches. Recent times, 
 this process has been widely studied and applied to the field of linking entities in the LOD realm, for instance using the DBPedia~\cite{Mendes:2011:DSS:2063518.2063519}. 
 Although there is no way of automatically creating a mapping with a 100\% of confidence (without human validation) a mapping under a certain percentage of confidence can be 
 enough for most of user-based services such as visualization. However, in case of using these techniques as previous step of a reasoning or 
 a formal verification process this ambiguity can lead to infer incorrect facts and must be avoided without a previous human validation. 

 On the other hand the use of semantics is also being applied to model organizational structures. In this case the notion 
 of corporate is presented in several vocabularies and ontologies as Dave Reynolds (Epimorphics Ltd) reports~\footnote{\url{http://www.epimorphics.com/web/wiki/organization-ontology-survey}}. 
 Currently the main effort is focused in the designed of the Organizations Vocabulary (a W3C Working Draft) in which the structure and 
 relationships of companies are being modelled. This proposal is especially relevant in the next aspects:  
 1) to unify existing models to provide a common specification; 2) to apply semantic web technologies and the Linked Data approach to enrich 
 and publish the relevant corporate information; 3) to provide access to the information via standard protocols 
 and 4)to  offer new services that can exploit this information to trace the evolution and behavior of the organization over time.

 
 \item Corporate Databases. Although corporate information such as identifier, name, economic activity, contact person, address or 
 financial status is usually publicly available in the official government registries the access to this valuable information can be 
 tedious due to different formats, query languages, etc. That is why other companies have emerged trying to index and exploit of 
 this public repositories selling reporting services that contains the aforementioned corporate information. Taking as 
 an example the Spanish Chambers of Commerce~\footnote{\url{http://www.camerdata.es/php/eng/fichero\_empresas.php}},
 Empresia.es~\footnote{\url{http://empresia.es}}, Axesor.es~\footnote{\url{http://www.axexor.es}} contain a database of companies and individual 
 entrepreneurs of Spain. This situation can be translated to the international scope, for instance Forbes keeps a list of 
 the most representative companies in different sectors. The underlying problem lies in the lack of unique identification, 
 names standardization and, as a consequence, difficulty of tracking company activity. In order to tackled these problems some 
 initiatives applying the LOD principles such as the Orgpedia~\footnote{\url{http://tw.rpi.edu/orgpedia/}} in United States or 
 ``The Open Database Of The Corporate World''~\footnote{\url{http://opencorporates.com/}} have scrapped and published the information 
 of companies creating a large database containing ($54,080,317$ of companies in May 2012) with high-valuable information like the company 
 identifier, e.g. Figure~\ref{figure:open} shows an example of an ``OpenCorporates'' organization. 
 Apart from that, reconciliation services have also been provided but the problem of mapping ($n$ string literals $\to$ $1$ company $\to$ $1$ URI, 
 as a human would do and the previous section has presented) still remains. Finally public web sites and major social networks such as Google 
 Places, Foursquare, Linkedin Companies, Facebook or Tuenti provide APIs and information managed by the own companies that is supposed 
 to be specially relevant to enrich existing corporate data once a company is uniquely identified.
 
  
 
\begin{figure}[!h]
\begin{center}
\begin{lstlisting}[language=SPARQL]
...
<http://opencorporates.com/id/companies/us_az/F07503757#id> 
	dct:created "1995-06-01"^^xsd:date;
	a adms:Identifer;
	skos:notation "F07503757";
	adms:schemaAgency "Arizona Corporation Commission".

<http://opencorporates.com/id/companies/us_az/F07503757#ra> 
	a locn:Address;
	locn:fullAddress "% CORPORATION SERVICE COMPANY, 
	2338 W ROYAL PALM RD STE-J, PHOENIX, AZ 85021".

<http://opencorporates.com/id/companies/us_az/F07503757> 
	opencorporates:companyType "CORPORATION";
	opencorporates:legalName "ORACLE SOFTWARE CORPORATION (FN)";
	a <http://s.opencalais.com/1/type/er/Company>,
		legal:LegalEntity;
	rdfs:label "ORACLE SOFTWARE CORPORATION (FN)";
	vCard:adr _:bnode1324364416;
	legal:companyType "CORPORATION";
	legal:legalIdentifier 
	  <http://opencorporates.com/id/companies/us_az/F07503757#id>;
	legal:legalName "ORACLE SOFTWARE CORPORATION (FN)";
	locn:registeredAddress 
	  <http://opencorporates.com/id/companies/us_az/F07503757#ra>.
...
\end{lstlisting}
\caption{Partial Information in the N3 format about an ``Oracle'' company in ``Open Corporates''.}
\label{figure:open}
\end{center}
\end{figure}
 
 
\end{itemize}

% 
 \section{The CORFU approach}
According to~\cite{Galvez2006,Morillo:2013:TAA:2424697.2424727} institutional name variations can be 
classified in two different groups: 1) Non-acceptable variations (affect to the meaning) due to mispelling or translation errors and 
2) acceptable variations (do not affect to the meaning) that correspond to different syntax forms such as abbreviations, use of acronyms or contextual 
information like country, suborganization, etc. In order to address these potential variations the CORFU (Company, ORganization and Firm Unifier) approach 
seeks for providing a stepwise method to unify corporate names using NLP and semantics based techniques as a previous step to perform 
an entity reconciliation process. Following some background and basic definitions are presented: 
\begin{itemize}
 \item A corporate name, $C_{str}$, is a string literal comprised of a set of concatenated descriptors (words, acronyms or abbreviations among others), $c_i$, 
containing different punctuation marks. 
\item A simplistic definition of a set filter can be: given a set $S$ and a predicate $\phi$, a filter $f$ is a function that generates 
a set $S_1 = \{ x : x \in S \wedge \phi (x)\}$.
\item A expanded set $S'$ can be defined as a superset of a given set $S$, $S \subseteq S'$.
\end{itemize}

The CORFU approach is based on the execution of common but customized steps in natural language processing applications that 
have been implemented using the Python NLTK API (unless another tool is named) and comprising: 1) text normalization; 2) filtering and 
3) comparison and clusterization. Furthermore and with the aim of linking the new corporate name $C'_{str}$ to an existing resource in the Web, in this particular case with OpenCorporates and the DBPedia. 
Thus the name is unified and can accomplish with the principles of the LOD initiative such as unique identification or data enrichment. 
In order to explain each of the steps performed in the CORFU unifier next tasks have been designed and implemented.

\begin{enumerate}
 \item Normalize raw text and remove duplicates. This step is comprised of: 1) remove strange characters and punctuation marks but keeping 
 those that are part of a word avoiding potential changes in  abbreviations or acronyms; 2) lowercase the raw text (although some semantics can 
 be lost previous works and empirical tests show that this is the best approach; 3) remove duplicates and 4)lemmatize the corporate name. 
 The implementation of this step to get the corporate name $C_{str}$ has ben performed using a combination of the aforemention API and 
 the Unix scripting tools AWK and SED. In this case, Figure~\ref{figure:step-1} presents 
 two relevant snippets of code for cleaning the name and making a basic a normalization of words. 
 
\begin{figure}[!h]
\begin{center}
\begin{lstlisting}[language=Python]        
rawname = filter(lambda x: x in string.letters or 
	      x in string.whitespace, line
...
def normalize(self, word):
  word = word.lower()
  word = self.lemmatizer.lemmatize(word)
  return word
\end{lstlisting}
\caption{Basic normalization of a word using the Python NLTK API.}
\label{figure:step-1}
\end{center}
\end{figure}
   
\item Filter the basic set of common stopwords in English. A common practice in NLP relies in the construction 
of stopwords sets that can filter some non-relevant words in a text. Nevertheless the use of this technique must 
consider two keypoints: 1) there is a common set of stopwords for any language than can be always used as a filter and 
2) depending on the context the set of stopwords should change to avoid the filtering of relevant words. In this particular 
case, a common and minimal set of stopwords in English,  $S_{english}$,  has been used. Thus the normalized corporate name $C_{str}$ is 
transformed in a new set of words $C^{1}_{str} = \{ c_i : c_i \in C_{str} \wedge c_i \notin S_{english}\}$. Figure~\ref{figure:step-2} presents 
the function for removing a set of words given a another set words, it can also be applied to other stages that require 
filtering capabilities.

\begin{figure}[!h]
\begin{center}
\begin{lstlisting}[language=Python] 
from nltk.corpus import stopwords
self.stop_words_wn = Set(stopwords.words('english'))
...
def remove_set(self, set,  name): 
  token_names= word_tokenize(name)       
  filtered_token_list = [w for w in  token_names if not w in set ]
  cleaned_name = " ".join(["".join(filtered_token) 
    for filtered_token in filtered_token_list])
  return cleaned_name
  
stop_unified_name = self.remove_set(self.stop_words_wn, name)
\end{lstlisting}
\caption{Filtering words with the Python NLTK API.}
\label{figure:step-2}
\end{center}
\end{figure}

\item Filter the expanded set of most common words in the dataset. Taking into account 
the aforementioned step this stage is based on the construction of a customized stopwords 
set of corporate names that is also expanded with Wordnet (ver. 3.0) synonyms. In order 
to create this set $S_{corporate}$ two strategies have been followed and applied: 1) create the set 
of words by hand (accurate but very time-consuming) and 2) extract automatically the set of ``most common words'' from the 
working dataset and make a hand-validation (less accurate and time-consuming). Figure~\ref{figure:step-3} partially 
shows these approaches implemented.
    
\begin{figure}[!h]
\begin{center}
\begin{lstlisting}[language=Python]  
from nltk.corpus import wordnet as wn
...
def create_syns_from_wn(self, word):
  syns = wn.synsets(word) 
  lemmas = Set()
  for syn in syns:
    lemmas = lemmas | Set([lemma.name for lemma in syn.lemmas] )
  return lemmas

def expand_list_wn(self,  list):    
  source = Set(list)
  expanded_set = Set()
  for word in source:
    expanded_set = expanded_set | self.create_syns_from_wn(word)
return expanded_set |  source

def list_most_used_words(companies, max):
	words = flatten(map(lambda company: company.rawname.split(), companies))
	counter = collections.Counter(words)  
	return [x[0].lower() for x in 
	  filter ( lambda x: x [1] > max, 
	 (itertools.islice(counter.most_common(), 0, 1000)))]
\end{lstlisting}
\caption{Expanding a list of words with Wordnet syns.}
\label{figure:step-3}
\end{center}
\end{figure}

\item Dictionary-based expansion of common acronyms and filtering. A dictionary of common acronyms in corporate 
names such as ``PTY'', ``LTD'' or ``PL'' and their variants has been created by hand in order to be able to extract and filter acronyms. 

\item Spellcheking (optional). This stage seeks for providing a method for fixing mispelling errors. It is based on the 
well-known speller~\cite{NorvigSpelling} of Peter Norvig that uses a train dataset for creating a classifier. Although the accuracy of this 
algorithm is pretty good for relevant words in corporate names, the empirical and unit tests with a working dataset 
have demonstrated that mispelled non-relevant words is more efficient and accurate using a stopwords set (this set has been 
built after checking if a word is not a ``most common word'' and exists in the Wordnet database). Furthermore some spelling corrections 
are not completely adequate for corporate names because words change and, therefore, a non-acceptable variant of the name 
could be included. That is why this stage is marked as optional and must be configured and performed with extreme care.

\item Pos-tagging parts of speech according to a grammar and filtering non-relevant parts of speech. The objective 
of this stage lies in ``classifying words into their parts of speech and labelling them accordingly is known as part-of-speech tagging''~\cite{LoperBird02}. In order 
to perform this task both a lemmatizer based on Wordnet and a grammar for corporate names (``NN''-nouns and ``JJ''-adjectives connected with articles and 
prepositions) have been designed, see Figure~\ref{figure:step-4}. Once words are tagged next step consists in filtering those non-relevant categories 
for corporate names keeping nouns and adjectives, as an example Figure~\ref{figure:step-5} shows how to walk and filter nodes in the parsed tree.

\begin{figure}[!h]
\begin{center}
\begin{lstlisting}[language=Python]  
self.lemmatizer = nltk.WordNetLemmatizer()
self.grammar = r"""
  NBAR: {<NN.*|JJ>*<NN.*>}   
  NP: {<NBAR>}
      {<NBAR><IN><NBAR>} 
    """
self.chunker = nltk.RegexpParser(self.grammar)
\end{lstlisting}
\caption{Regular expression based chunker in Python NLTK.}
\label{figure:step-4}
\end{center}
\end{figure}   
  
 \begin{figure}[!h]
\begin{center}
\begin{lstlisting}[language=Python]  
def leaves(self, tree):
  for subtree in tree.subtrees(filter = lambda t: t.node=='NP'):
    yield subtree.leaves()
\end{lstlisting}
\caption{Filtering words by the category ``NP'' (noun phrase).}
\label{figure:step-5}
\end{center}
\end{figure}      

\item Create a matrix between potential target names using a string comparison function and 
iterate $n$ times until clustering or align with ``the most probably/used name'' 
in the working dataset. Since previous steps have cleaned and prepared the raw corporate name 
it is now necessary to group the corporate names and generate a final unified name. Due to the 
process can be highly customized a large number of names are now more similar than in 
the begining. In order to compare between the similarity of a company name with the whole set of company names 
a matrix $M$ containing in each position $M(i,j)$ a value (0-100) is created. 
Thus given a company in the position  calculated by comparing with a function the names of 
companies $i$ and $j$ 

\item Reconcile the generated corporate name with an existing reconcile service.

\end{enumerate}

\subsection{Use Case: Corporate names in the Australian e-Procurement domain }
% 
\begin{enumerate}
 \item FIXME
 \item  For instance, in the case of 1) the set of customized by hand stopwords contains some words such as ``Advertising'' or ``Ordnance'' (some mispelled words 
have been included here to avoid false positives in the spelling phase). On the other hand, the first $max$ most used words in the corporate names dataset are initially selected as stopwords, 
in this particular case and after studying the working dataset, the first $50$ words have been automatically selected containing words such as ``Management'', ``Services'' or ``Associates'' that 
are very common in corporate names and, therefore, non-relevant.
\end{enumerate}

\section{Evaluation}

\subsection{Research design}
% Since this recommender engine is designed to be used by professionals 
% in the journalism domain rather than regular users the purpose of this study 
% is to compare a set of news suggested by the recommendation engine with the 
% expected results of a panel of experts. Thus, the objective is 
% to asses if recommendations provided by Wesomender can fulfill 
% the expectations and requirements of professionals in this sector. 
% 
\subsection{Sample}
In order to validate the approach outlined in this summary the dataset of supplier names in Australia in the period 2005-2012 containing $430188$ full names 
and $77526$ unique names has been selected. Initially the traditional reconciliation process using 
Google Refine and OpenCorporates generated an $8$\% of links but most of them were 
incorrect or not grouped by the same resource. After applying the CORFU approach 
the initial set $77526$ names were grouped in $40278$ distinct names 
($51$\% of potential right links to OpenCoporates). Furthermore these unified names were manually 
reviewed by hand and in the specific case of the first one hundred companies in the Forbes 
list a $100$\% of correct names can be now ensured.
 
\subsection{Results and Discussion}


\section{Conclusions and Future Work}
The main conclusion of this works lies in the design of a technique to prepare 
raw organization names in a specific context, e.g. Australia supplier names,  
before performing a reconciliation process. Although the percentage of potential 
right links to existing datasets has been dramatically improved it is clear that 
human-validation is also required to ensure the correct unification of names. 
Other NLP techniques based on n-grams or a classifier will be used in the future 
to deliver a complete and intelligent company unifier. On the other hand, the 
application of this technique enables the comparison of rewarded contracts to 
different companies and can help to improve the transparency in public 
administrations.

\bibliographystyle{plain}
% %\bibliographystyle{unsrt}
% %\bibliographystyle{acm}
\bibliography{references}
% \renewcommand{\bibname}{References}
\end{document}

